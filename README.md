# BIG DTA
            Nama  : Annisa Fitri Yuliandra
            NIM   : 2041720123
            Kelas : TI 3B
            Prodi : D4 Teknik Informatika

---
## TUGAS PRAKTIKUM

1.  mylist: Variabel yang digunakan untuk menyimpan sebuah list (daftar) data di dalam bahasa pemrograman Python.
2.	myschema: Variabel yang digunakan untuk menyimpan sebuah schema atau struktur data di dalam bahasa pemrograman PySpark.
3.	spark.createDataFrame: Metode yang digunakan untuk membuat sebuah DataFrame di dalam Spark.
4.	parallelize: Metode yang digunakan untuk membuat sebuah RDD (Resilient Distributed Dataset) dari sebuah list atau koleksi data.
5.	toDF: Metode yang digunakan untuk mengubah sebuah RDD menjadi sebuah DataFrame di dalam PySpark.
6.	hadoop: Sebuah proyek open-source Apache yang terdiri dari beberapa perangkat lunak terkait Big Data dan Hadoop Distributed File System (HDFS).
7.	fs: Variabel yang digunakan untuk memanggil file system yang digunakan oleh Hadoop.
8.	put: Metode yang digunakan untuk mengunggah sebuah file ke dalam file system yang digunakan oleh Hadoop.
9.	pyspark.sql: Paket Python yang digunakan untuk memanipulasi data secara terdistribusi menggunakan PySpark.
10.	SQLContext: Kelas yang digunakan untuk membuat sebuah koneksi dengan database SQL di dalam PySpark.
11.	createOrReplaceTempView: Metode yang digunakan untuk membuat sebuah tampilan sementara (temporary view) dari sebuah DataFrame atau tabel di dalam PySpark.
12.	show: Metode yang digunakan untuk menampilkan sebuah DataFrame atau tabel di dalam PySpark.
13.	textFile: Metode yang digunakan untuk membaca sebuah file teks di dalam PySpark.
14.	map: Metode yang digunakan untuk menerapkan sebuah fungsi ke setiap elemen di dalam RDD di dalam PySpark.
15.	lambda: Sebuah ekspresi Python yang digunakan untuk membuat sebuah fungsi anonim.
16.	strip: Metode yang digunakan untuk menghilangkan spasi atau karakter kosong di awal atau akhir sebuah string di dalam Python.
17.	StructField: Kelas yang digunakan untuk mendefinisikan sebuah field dalam sebuah schema di dalam PySpark.
18.	StringType: Tipe data string di dalam PySpark.
19.	spark.read.format: Metode yang digunakan untuk membaca sebuah file dengan format tertentu di dalam Spark.
20.	jdbc: Metode yang digunakan untuk membaca sebuah tabel dari database menggunakan JDBC didalam PySpark.
21.	options: Argumen yang digunakan untuk menyediakan konfigurasi tambahan ketika membaca sebuah tabel dari database menggunakan JDBC di dalam PySpark.
22.	load: Metode yang digunakan untuk membaca sebuah file di dalam Spark.
23.	collect: Metode yang digunakan untuk mengumpulkan data dari semua partisi RDD ke dalam sebuah list.
24.	rdd: Variabel yang digunakan untuk menyimpan sebuah RDD di dalam Spark.
25.	take: Metode yang digunakan untuk mengambil n elemen pertama dari sebuah RDD di dalam Spark.
26.	makeRDD: Metode yang digunakan untuk membuat sebuah RDD di dalam Spark dari sebuah list.
27.	Seq: Tipe data urutan (sequence) di dalam Scala.
28.	createDataset: Metode yang digunakan untuk membuat sebuah Dataset di dalam PySpark.
29.	filter: Metode yang digunakan untuk menyaring (filter) elemen dari sebuah RDD berdasarkan kondisi tertentu di dalam PySpark.
30.	as: Mengubah nama kolom pada sebuah DataFrame.
31.	toDF: Mengubah sebuah RDD menjadi sebuah DataFrame.
32.	first: Mengambil nilai pertama pada sebuah DataFrame atau kolom.
33.	listDatabases: Menampilkan list dari database yang ada pada SparkSQL.
34.	listTables: Menampilkan list dari tabel yang ada pada SparkSQL.
35.	listFunctions: Menampilkan list dari fungsi yang ada pada SparkSQL.
36.	isCached: Mengecek apakah sebuah DataFrame sudah di-cache atau belum.
37.	select: Mengambil kolom tertentu dari sebuah DataFrame.
38.	read: Membaca data dari sebuah lokasi atau file ke dalam DataFrame.
39.	text: Membaca file teks menjadi RDD.
40.	load: Memuat data dari sebuah lokasi atau file menjadi DataFrame.
41.	json: Membaca file JSON menjadi DataFrame.
42.	format: Menentukan format yang digunakan untuk membaca atau menulis data.
43.	printSchema: Menampilkan skema dari sebuah DataFrame.
44.	write: Menulis data dari sebuah DataFrame ke dalam sebuah file atau lokasi.
45.	save: Menyimpan data dari sebuah DataFrame ke dalam sebuah file atau lokasi.
46.	parquet: Menggunakan format Parquet untuk membaca atau menulis data.
47.	Options: Menentukan opsi saat membaca atau menulis data.
48.	inferSchema: Memperkirakan skema DataFrame saat membaca data.
49.	csv: Membaca atau menulis file CSV.
50.	header: Menentukan apakah file CSV memiliki header atau tidak.
51.	codec: Menentukan codec yang digunakan untuk membaca atau menulis data.